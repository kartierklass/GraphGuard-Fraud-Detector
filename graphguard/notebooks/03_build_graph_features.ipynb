{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80cedcfe",
   "metadata": {},
   "source": [
    "# 03. Build Graph Features\n",
    "\n",
    "This notebook builds a transaction graph from our fraud detection data and computes graph metrics.\n",
    "We'll create features that capture the network structure of transactions to help identify fraud patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4377dd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd299118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Data directory: ..\\app\\artifacts\n",
      "📁 Output directory: ..\\app\\artifacts\n"
     ]
    }
   ],
   "source": [
    "# Setup paths\n",
    "data_dir = Path(\"../app/artifacts\")\n",
    "output_dir = Path(\"../app/artifacts\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"📁 Data directory: {data_dir}\")\n",
    "print(f\"📁 Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba386355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loading processed data...\n",
      "✅ Data loaded successfully\n",
      "📊 Shape: (118108, 434)\n",
      "📊 Columns: 434\n",
      "📊 Memory usage: 502.80 MB\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "print(\"📊 Loading processed data...\")\n",
    "\n",
    "data_path = data_dir / \"eda_processed_data.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"✅ Data loaded successfully\")\n",
    "print(f\"📊 Shape: {df.shape}\")\n",
    "print(f\"📊 Columns: {len(df.columns)}\")\n",
    "print(f\"📊 Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "157e3af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Data Overview:\n",
      "==================================================\n",
      "Total transactions: 118,108\n",
      "Fraud rate: 0.0350\n",
      "Fraud count: 4,133\n",
      "Legitimate count: 113,975\n"
     ]
    }
   ],
   "source": [
    "# Display data info\n",
    "print(\"📋 Data Overview:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total transactions: {len(df):,}\")\n",
    "print(f\"Fraud rate: {df['isFraud'].mean():.4f}\")\n",
    "print(f\"Fraud count: {df['isFraud'].sum():,}\")\n",
    "print(f\"Legitimate count: {(df['isFraud'] == 0).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "875b906b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "�� Key entity columns: ['card1', 'addr1', 'P_emaildomain', 'R_emaildomain', 'DeviceType', 'ProductCD']\n",
      "\n",
      "📊 Sample of key columns:\n",
      "   card1  addr1 P_emaildomain R_emaildomain DeviceType ProductCD\n",
      "0  14223  204.0     gmail.com           NaN        NaN         W\n",
      "1   2516  315.0     yahoo.com           NaN        NaN         W\n",
      "2   7585  272.0     yahoo.com           NaN        NaN         W\n",
      "3  10823    NaN     gmail.com     gmail.com     mobile         C\n",
      "4   9633    NaN   hotmail.com   hotmail.com     mobile         C\n"
     ]
    }
   ],
   "source": [
    "# Show sample of key columns\n",
    "key_cols = ['card1', 'addr1', 'P_emaildomain', 'R_emaildomain', 'DeviceType', 'ProductCD']\n",
    "print(f\"\\n�� Key entity columns: {key_cols}\")\n",
    "print(\"\\n📊 Sample of key columns:\")\n",
    "print(df[key_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36b3a94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Building transaction graph...\n"
     ]
    }
   ],
   "source": [
    "# Build Graph\n",
    "print(\"🔗 Building transaction graph...\")\n",
    "\n",
    "# Initialize the graph\n",
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a73271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "��️ Using 8 entity columns: ['card1', 'addr1', 'P_emaildomain', 'R_emaildomain', 'DeviceType', 'ProductCD', 'card4', 'card6']\n"
     ]
    }
   ],
   "source": [
    "# Define entity columns to use for graph construction\n",
    "entity_cols = [\n",
    "    'card1', 'addr1', 'P_emaildomain', 'R_emaildomain', \n",
    "    'DeviceType', 'ProductCD', 'card4', 'card6'\n",
    "]\n",
    "\n",
    "print(f\"��️ Using {len(entity_cols)} entity columns: {entity_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66acc4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing transaction 0/118,108\n",
      "  Processing transaction 10,000/118,108\n",
      "  Processing transaction 20,000/118,108\n",
      "  Processing transaction 30,000/118,108\n",
      "  Processing transaction 40,000/118,108\n",
      "  Processing transaction 50,000/118,108\n",
      "  Processing transaction 60,000/118,108\n",
      "  Processing transaction 70,000/118,108\n",
      "  Processing transaction 80,000/118,108\n",
      "  Processing transaction 90,000/118,108\n",
      "  Processing transaction 100,000/118,108\n",
      "  Processing transaction 110,000/118,108\n"
     ]
    }
   ],
   "source": [
    "# Add nodes and edges\n",
    "transaction_count = 0\n",
    "for idx, row in df.iterrows():\n",
    "    if idx % 10000 == 0:\n",
    "        print(f\"  Processing transaction {idx:,}/{len(df):,}\")\n",
    "    \n",
    "    # Get entities for this transaction\n",
    "    entities = []\n",
    "    for col in entity_cols:\n",
    "        if pd.notna(row[col]) and str(row[col]) != 'nan':\n",
    "            entity_id = f\"{col}_{str(row[col])}\"\n",
    "            entities.append(entity_id)\n",
    "    \n",
    "    # Add nodes\n",
    "    for entity in entities:\n",
    "        G.add_node(entity)\n",
    "    \n",
    "    # Add edges between all entities in this transaction\n",
    "    for i in range(len(entities)):\n",
    "        for j in range(i + 1, len(entities)):\n",
    "            G.add_edge(entities[i], entities[j])\n",
    "    \n",
    "    transaction_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27567257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Graph construction completed!\n",
      "�� Total transactions processed: 118,108\n",
      "📊 Graph nodes: 8,491\n",
      "📊 Graph edges: 81,857\n"
     ]
    }
   ],
   "source": [
    "print(f\"✅ Graph construction completed!\")\n",
    "print(f\"�� Total transactions processed: {transaction_count:,}\")\n",
    "print(f\"📊 Graph nodes: {G.number_of_nodes():,}\")\n",
    "print(f\"📊 Graph edges: {G.number_of_edges():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4440acdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Analyzing graph structure...\n",
      "🔍 Basic Graph Statistics:\n",
      "========================================\n",
      "Number of nodes: 8,491\n",
      "Number of edges: 81,857\n",
      "Number of connected components: 1\n",
      "Density: 0.002271\n",
      "Largest component size: 8,491\n",
      "Average degree: 19.28\n",
      "Max degree: 5850\n",
      "Min degree: 2\n"
     ]
    }
   ],
   "source": [
    "# Graph Analysis\n",
    "print(\"📊 Analyzing graph structure...\")\n",
    "\n",
    "# Basic graph statistics\n",
    "print(\"🔍 Basic Graph Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Number of nodes: {G.number_of_nodes():,}\")\n",
    "print(f\"Number of edges: {G.number_of_edges():,}\")\n",
    "print(f\"Number of connected components: {nx.number_connected_components(G)}\")\n",
    "print(f\"Density: {nx.density(G):.6f}\")\n",
    "\n",
    "# Largest connected component\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "print(f\"Largest component size: {len(largest_cc):,}\")\n",
    "\n",
    "# Degree distribution\n",
    "degrees = [d for n, d in G.degree()]\n",
    "print(f\"Average degree: {np.mean(degrees):.2f}\")\n",
    "print(f\"Max degree: {max(degrees)}\")\n",
    "print(f\"Min degree: {min(degrees)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57f35d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 Calculating graph metrics...\n"
     ]
    }
   ],
   "source": [
    "# Calculate Graph Metrics\n",
    "print(\"🧮 Calculating graph metrics...\")\n",
    "\n",
    "# Initialize metrics storage\n",
    "metrics_data = {}\n",
    "\n",
    "# Calculate metrics for each node\n",
    "node_count = 0\n",
    "total_nodes = G.number_of_nodes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90a03692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 Calculating essential graph metrics only...\n",
      "  Calculating degree centrality...\n",
      "  Calculating simple PageRank...\n",
      "  Skipping expensive metrics (clustering & betweenness)...\n",
      "✅ Essential metrics calculated in seconds!\n",
      "✅ Metrics stored for 8,491 nodes\n"
     ]
    }
   ],
   "source": [
    "# Calculate Graph Metrics (ULTRA-MINIMAL - FASTEST VERSION)\n",
    "print(\"🧮 Calculating essential graph metrics only...\")\n",
    "\n",
    "print(\"  Calculating degree centrality...\")\n",
    "degree_cent = dict(G.degree())\n",
    "\n",
    "print(\"  Calculating simple PageRank...\")\n",
    "# Use very fast PageRank with minimal iterations\n",
    "pagerank = nx.pagerank(G, alpha=0.85, max_iter=20, tol=1e-2)\n",
    "\n",
    "print(\"  Skipping expensive metrics (clustering & betweenness)...\")\n",
    "# Skip slow metrics entirely for large graphs\n",
    "clustering = {node: 0.0 for node in G.nodes()}\n",
    "betweenness = {node: 0.0 for node in G.nodes()}\n",
    "\n",
    "print(\"✅ Essential metrics calculated in seconds!\")\n",
    "\n",
    "# Create metrics dictionary\n",
    "metrics_data = {}\n",
    "for node in G.nodes():\n",
    "    metrics_data[node] = {\n",
    "        'degree_centrality': degree_cent.get(node, 0),\n",
    "        'pagerank': pagerank.get(node, 0.0),\n",
    "        'clustering_coefficient': clustering.get(node, 0.0),\n",
    "        'betweenness_centrality': betweenness.get(node, 0.0)\n",
    "    }\n",
    "\n",
    "print(f\"✅ Metrics stored for {len(metrics_data):,} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb80e385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Calculating clustering coefficient (optimized)...\n",
      "    Computing clustering for 8,056 high-degree nodes...\n",
      "  Skipping betweenness (still too slow)...\n",
      "✅ Optimized metrics calculated!\n"
     ]
    }
   ],
   "source": [
    "print(\"  Calculating clustering coefficient (optimized)...\")\n",
    "# Calculate clustering for high-degree nodes only (most important)\n",
    "high_degree_nodes = [node for node, degree in degree_cent.items() if degree >= 5]\n",
    "print(f\"    Computing clustering for {len(high_degree_nodes):,} high-degree nodes...\")\n",
    "\n",
    "clustering_partial = nx.clustering(G, nodes=high_degree_nodes)\n",
    "# Fill remaining nodes with 0\n",
    "clustering = {node: clustering_partial.get(node, 0.0) for node in G.nodes()}\n",
    "\n",
    "print(\"  Skipping betweenness (still too slow)...\")\n",
    "betweenness = {node: 0.0 for node in G.nodes()}\n",
    "\n",
    "print(\"✅ Optimized metrics calculated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02c0dd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Calculating clustering for high-degree nodes...\n",
      "  Calculating sampled betweenness centrality...\n",
      "    Computing betweenness for top 1000 nodes...\n",
      "✅ All metrics calculated with smart sampling!\n"
     ]
    }
   ],
   "source": [
    "print(\"  Calculating clustering for high-degree nodes...\")\n",
    "high_degree_nodes = [node for node, degree in degree_cent.items() if degree >= 5]\n",
    "clustering_partial = nx.clustering(G, nodes=high_degree_nodes)\n",
    "clustering = {node: clustering_partial.get(node, 0.0) for node in G.nodes()}\n",
    "\n",
    "print(\"  Calculating sampled betweenness centrality...\")\n",
    "# Sample only 1000 most important nodes for betweenness\n",
    "top_nodes = sorted(degree_cent.items(), key=lambda x: x[1], reverse=True)[:1000]\n",
    "sample_nodes = [node for node, _ in top_nodes]\n",
    "\n",
    "print(f\"    Computing betweenness for top {len(sample_nodes)} nodes...\")\n",
    "betweenness_sample = nx.betweenness_centrality(G, k=len(sample_nodes))\n",
    "\n",
    "# Assign betweenness values\n",
    "betweenness = {}\n",
    "for node in G.nodes():\n",
    "    if node in betweenness_sample:\n",
    "        betweenness[node] = betweenness_sample[node]\n",
    "    else:\n",
    "        # Estimate based on degree for non-sampled nodes\n",
    "        betweenness[node] = degree_cent[node] / (G.number_of_nodes() * 1000)\n",
    "\n",
    "print(\"✅ All metrics calculated with smart sampling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1036c96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Metrics stored for 8,491 nodes\n"
     ]
    }
   ],
   "source": [
    "# Create metrics dictionary\n",
    "metrics_data = {}\n",
    "for node in G.nodes():\n",
    "    metrics_data[node] = {\n",
    "        'degree_centrality': degree_cent.get(node, 0),\n",
    "        'pagerank': pagerank.get(node, 0.0),\n",
    "        'clustering_coefficient': clustering.get(node, 0.0),\n",
    "        'betweenness_centrality': betweenness.get(node, 0.0)\n",
    "    }\n",
    "\n",
    "print(f\"✅ Metrics stored for {len(metrics_data):,} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2001905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Creating features DataFrame...\n",
      "✅ Graph features DataFrame created\n",
      "📊 Shape: (8491, 5)\n",
      "📊 Columns: ['node', 'degree_centrality', 'pagerank', 'clustering_coefficient', 'betweenness_centrality']\n",
      "\n",
      "📋 Sample of graph features:\n",
      "                      node  degree_centrality  pagerank  \\\n",
      "0              card1_14223                 11  0.000019   \n",
      "1              addr1_204.0               1055  0.008494   \n",
      "2  P_emaildomain_gmail.com               4990  0.061364   \n",
      "3              ProductCD_W               5850  0.087432   \n",
      "4               card4_visa               4847  0.069541   \n",
      "5              card6_debit               5541  0.081945   \n",
      "6               card1_2516                  5  0.000018   \n",
      "7              addr1_315.0                755  0.007064   \n",
      "8  P_emaildomain_yahoo.com               3124  0.034685   \n",
      "9         card4_mastercard               3081  0.043342   \n",
      "\n",
      "   clustering_coefficient  betweenness_centrality  \n",
      "0                0.836364            2.468695e-09  \n",
      "1                0.024324            4.835185e-03  \n",
      "2                0.003461            1.301770e-01  \n",
      "3                0.002523            1.868879e-01  \n",
      "4                0.003682            1.397444e-01  \n",
      "5                0.002981            1.770176e-01  \n",
      "6                1.000000            0.000000e+00  \n",
      "7                0.031236            2.740056e-03  \n",
      "8                0.006416            4.429546e-02  \n",
      "9                0.005665            5.810580e-02  \n"
     ]
    }
   ],
   "source": [
    "# Create Features DataFrame\n",
    "print(\"📊 Creating features DataFrame...\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "graph_features_df = pd.DataFrame.from_dict(metrics_data, orient='index')\n",
    "\n",
    "# Reset index to get node as a column\n",
    "graph_features_df.reset_index(inplace=True)\n",
    "graph_features_df.rename(columns={'index': 'node'}, inplace=True)\n",
    "\n",
    "print(f\"✅ Graph features DataFrame created\")\n",
    "print(f\"📊 Shape: {graph_features_df.shape}\")\n",
    "print(f\"📊 Columns: {graph_features_df.columns.tolist()}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n📋 Sample of graph features:\")\n",
    "print(graph_features_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5e07834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Graph Features Statistics:\n",
      "==================================================\n",
      "\n",
      "🔍 degree_centrality:\n",
      "  Mean: 19.280886\n",
      "  Std: 154.510367\n",
      "  Min: 2.000000\n",
      "  Max: 5850.000000\n",
      "  Missing: 0\n",
      "\n",
      "🔍 pagerank:\n",
      "  Mean: 0.000118\n",
      "  Std: 0.001951\n",
      "  Min: 0.000018\n",
      "  Max: 0.087432\n",
      "  Missing: 0\n",
      "\n",
      "🔍 clustering_coefficient:\n",
      "  Mean: 0.854908\n",
      "  Std: 0.246927\n",
      "  Min: 0.000000\n",
      "  Max: 1.000000\n",
      "  Missing: 0\n",
      "\n",
      "🔍 betweenness_centrality:\n",
      "  Mean: 0.000127\n",
      "  Std: 0.003725\n",
      "  Min: 0.000000\n",
      "  Max: 0.186888\n",
      "  Missing: 0\n",
      "\n",
      "⚠️ Total missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Feature Statistics\n",
    "print(\"📊 Graph Features Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display statistics for each metric\n",
    "for col in graph_features_df.columns:\n",
    "    if col != 'node':\n",
    "        print(f\"\\n🔍 {col}:\")\n",
    "        print(f\"  Mean: {graph_features_df[col].mean():.6f}\")\n",
    "        print(f\"  Std: {graph_features_df[col].std():.6f}\")\n",
    "        print(f\"  Min: {graph_features_df[col].min():.6f}\")\n",
    "        print(f\"  Max: {graph_features_df[col].max():.6f}\")\n",
    "        print(f\"  Missing: {graph_features_df[col].isnull().sum()}\")\n",
    "\n",
    "# Check for any missing values\n",
    "missing_count = graph_features_df.isnull().sum().sum()\n",
    "print(f\"\\n⚠️ Total missing values: {missing_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "594f7180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Handling missing values...\n",
      "✅ Missing values filled\n",
      "📊 Final shape: (8491, 5)\n",
      "📊 Memory usage: 0.74 MB\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "print(\"🔧 Handling missing values...\")\n",
    "\n",
    "# Fill missing values with 0 (appropriate for graph metrics)\n",
    "graph_features_df = graph_features_df.fillna(0)\n",
    "\n",
    "print(f\"✅ Missing values filled\")\n",
    "print(f\"📊 Final shape: {graph_features_df.shape}\")\n",
    "print(f\"📊 Memory usage: {graph_features_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "871fe3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving graph features...\n",
      "✅ Graph features saved to: ..\\app\\artifacts\\graph_features.parquet\n",
      "�� File size: 0.17 MB\n"
     ]
    }
   ],
   "source": [
    "# Save Artifacts\n",
    "print(\"💾 Saving graph features...\")\n",
    "\n",
    "# Save as parquet\n",
    "output_path = output_dir / \"graph_features.parquet\"\n",
    "graph_features_df.to_parquet(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Graph features saved to: {output_path}\")\n",
    "print(f\"�� File size: {output_path.stat().st_size / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1521281f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV backup saved to: ..\\app\\artifacts\\graph_features.csv\n",
      "\n",
      "�� GRAPH FEATURES CREATION COMPLETED!\n",
      "📊 Total nodes with features: 8,491\n",
      "�� Features per node: 4\n",
      "\n",
      "Next step: Use these graph features to enhance your XGBoost model!\n"
     ]
    }
   ],
   "source": [
    "# Also save as CSV for inspection (optional)\n",
    "csv_path = output_dir / \"graph_features.csv\"\n",
    "graph_features_df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV backup saved to: {csv_path}\")\n",
    "\n",
    "print(\"\\n�� GRAPH FEATURES CREATION COMPLETED!\")\n",
    "print(f\"📊 Total nodes with features: {len(graph_features_df):,}\")\n",
    "print(f\"�� Features per node: {len(graph_features_df.columns) - 1}\")\n",
    "print(f\"\\nNext step: Use these graph features to enhance your XGBoost model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20fcc6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "�� Final Verification:\n",
      "========================================\n",
      "✅ Output file exists: ..\\app\\artifacts\\graph_features.parquet\n",
      "�� File size: 0.17 MB\n",
      "\n",
      "📋 Final sample of graph features:\n",
      "                      node  degree_centrality  pagerank  \\\n",
      "0              card1_14223                 11  0.000019   \n",
      "1              addr1_204.0               1055  0.008494   \n",
      "2  P_emaildomain_gmail.com               4990  0.061364   \n",
      "3              ProductCD_W               5850  0.087432   \n",
      "4               card4_visa               4847  0.069541   \n",
      "\n",
      "   clustering_coefficient  betweenness_centrality  \n",
      "0                0.836364            2.468695e-09  \n",
      "1                0.024324            4.835185e-03  \n",
      "2                0.003461            1.301770e-01  \n",
      "3                0.002523            1.868879e-01  \n",
      "4                0.003682            1.397444e-01  \n",
      "\n",
      "�� Ready for the next step: Hybrid model training!\n",
      "These graph features will be merged with transaction features to create a powerful hybrid model.\n"
     ]
    }
   ],
   "source": [
    "# Verification and Summary\n",
    "print(\"�� Final Verification:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check file was created\n",
    "if output_path.exists():\n",
    "    print(f\"✅ Output file exists: {output_path}\")\n",
    "    print(f\"�� File size: {output_path.stat().st_size / 1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(f\"❌ Output file not found!\")\n",
    "\n",
    "# Display final sample\n",
    "print(f\"\\n📋 Final sample of graph features:\")\n",
    "print(graph_features_df.head())\n",
    "\n",
    "print(f\"\\n�� Ready for the next step: Hybrid model training!\")\n",
    "print(f\"These graph features will be merged with transaction features to create a powerful hybrid model.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
