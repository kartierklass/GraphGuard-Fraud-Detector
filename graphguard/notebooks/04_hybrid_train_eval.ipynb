{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f0479e",
   "metadata": {},
   "source": [
    "# 04. Hybrid Model Training & Evaluation\n",
    "\n",
    "This notebook trains our final hybrid fraud detection model by combining:\n",
    "- Traditional transaction features from our baseline model\n",
    "- Graph network features from our transaction graph\n",
    "\n",
    "Our goal is to beat the baseline ROC-AUC score of **0.9213** using graph-enhanced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3bebd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "379f9de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Data directory: ..\\app\\artifacts\n",
      "📁 Output directory: ..\\app\\artifacts\n",
      "🎯 Baseline ROC-AUC to beat: 0.9213\n"
     ]
    }
   ],
   "source": [
    "# Setup paths\n",
    "data_dir = Path(\"../app/artifacts\")\n",
    "output_dir = Path(\"../app/artifacts\")\n",
    "\n",
    "print(f\"📁 Data directory: {data_dir}\")\n",
    "print(f\"📁 Output directory: {output_dir}\")\n",
    "\n",
    "# Define baseline score to beat\n",
    "BASELINE_ROC_AUC = 0.9213\n",
    "print(f\"🎯 Baseline ROC-AUC to beat: {BASELINE_ROC_AUC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8cbca72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loading datasets...\n",
      "✅ Transaction data loaded: (118108, 434)\n",
      "✅ Graph features loaded: (8491, 5)\n",
      "📊 Transaction columns: 434\n",
      "📊 Graph feature columns: ['node', 'degree_centrality', 'pagerank', 'clustering_coefficient', 'betweenness_centrality']\n"
     ]
    }
   ],
   "source": [
    "# Load Datasets\n",
    "print(\"📊 Loading datasets...\")\n",
    "\n",
    "# Load main transaction data\n",
    "transaction_path = data_dir / \"eda_processed_data.csv\"\n",
    "df_transactions = pd.read_csv(transaction_path)\n",
    "\n",
    "# Load graph features\n",
    "graph_path = data_dir / \"graph_features.parquet\"\n",
    "df_graph = pd.read_parquet(graph_path)\n",
    "\n",
    "print(f\"✅ Transaction data loaded: {df_transactions.shape}\")\n",
    "print(f\"✅ Graph features loaded: {df_graph.shape}\")\n",
    "print(f\"📊 Transaction columns: {len(df_transactions.columns)}\")\n",
    "print(f\"📊 Graph feature columns: {df_graph.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e6fb10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Inspecting data for merging...\n",
      "\n",
      "📊 Sample transaction entities:\n",
      "   card1  addr1 P_emaildomain R_emaildomain DeviceType ProductCD\n",
      "0  14223  204.0     gmail.com           NaN        NaN         W\n",
      "1   2516  315.0     yahoo.com           NaN        NaN         W\n",
      "2   7585  272.0     yahoo.com           NaN        NaN         W\n",
      "3  10823    NaN     gmail.com     gmail.com     mobile         C\n",
      "4   9633    NaN   hotmail.com   hotmail.com     mobile         C\n",
      "\n",
      "📊 Sample graph features:\n",
      "                      node  degree_centrality  pagerank  \\\n",
      "0              card1_14223                 11  0.000019   \n",
      "1              addr1_204.0               1055  0.008494   \n",
      "2  P_emaildomain_gmail.com               4990  0.061364   \n",
      "3              ProductCD_W               5850  0.087432   \n",
      "4               card4_visa               4847  0.069541   \n",
      "\n",
      "   clustering_coefficient  betweenness_centrality  \n",
      "0                0.836364            2.468695e-09  \n",
      "1                0.024324            4.835185e-03  \n",
      "2                0.003461            1.301770e-01  \n",
      "3                0.002523            1.868879e-01  \n",
      "4                0.003682            1.397444e-01  \n",
      "\n",
      "🔍 Graph node patterns:\n",
      "0\n",
      "card1         8185\n",
      "addr1          174\n",
      "P               59\n",
      "R               58\n",
      "ProductCD        5\n",
      "card4            4\n",
      "card6            4\n",
      "DeviceType       2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect data for merging\n",
    "print(\"🔍 Inspecting data for merging...\")\n",
    "\n",
    "# Show sample of transaction data key columns\n",
    "key_entity_cols = ['card1', 'addr1', 'P_emaildomain', 'R_emaildomain', 'DeviceType', 'ProductCD']\n",
    "print(\"\\n📊 Sample transaction entities:\")\n",
    "print(df_transactions[key_entity_cols].head())\n",
    "\n",
    "# Show sample of graph features\n",
    "print(\"\\n📊 Sample graph features:\")\n",
    "print(df_graph.head())\n",
    "\n",
    "# Check graph node patterns\n",
    "print(f\"\\n🔍 Graph node patterns:\")\n",
    "node_types = df_graph['node'].str.split('_', n=1, expand=True)[0].value_counts()\n",
    "print(node_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "574b5fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Merging graph features with transaction data...\n",
      "🎯 Primary entities for merging: ['card1', 'addr1', 'P_emaildomain']\n",
      "🎯 Secondary entities for merging: ['ProductCD', 'DeviceType', 'card4']\n"
     ]
    }
   ],
   "source": [
    "# Merge Features - Key Step!\n",
    "print(\"🔗 Merging graph features with transaction data...\")\n",
    "\n",
    "# Create a copy to work with\n",
    "df_merged = df_transactions.copy()\n",
    "\n",
    "# Define primary and secondary entities for merging\n",
    "primary_entities = ['card1', 'addr1', 'P_emaildomain']  # Most important for fraud\n",
    "secondary_entities = ['ProductCD', 'DeviceType', 'card4']  # Secondary entities\n",
    "\n",
    "print(f\"🎯 Primary entities for merging: {primary_entities}\")\n",
    "print(f\"🎯 Secondary entities for merging: {secondary_entities}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e41b75c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge function\n",
    "def merge_graph_features(df, entity_col, suffix):\n",
    "    \"\"\"Merge graph features for a specific entity column\"\"\"\n",
    "    # Create entity IDs that match graph node format\n",
    "    entity_ids = df[entity_col].astype(str).apply(lambda x: f\"{entity_col}_{x}\" if pd.notna(x) and x != 'nan' else None)\n",
    "    \n",
    "    # Create temporary dataframe for merging\n",
    "    temp_df = pd.DataFrame({\n",
    "        'entity_id': entity_ids,\n",
    "        'index': df.index\n",
    "    }).dropna()\n",
    "    \n",
    "    # Merge with graph features\n",
    "    merged = temp_df.merge(\n",
    "        df_graph.rename(columns={'node': 'entity_id'}), \n",
    "        on='entity_id', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Rename columns with suffix\n",
    "    feature_cols = ['degree_centrality', 'pagerank', 'clustering_coefficient', 'betweenness_centrality']\n",
    "    for col in feature_cols:\n",
    "        merged[f\"{col}_{suffix}\"] = merged[col]\n",
    "    \n",
    "    # Return only the new features aligned with original dataframe\n",
    "    result = merged.set_index('index')[feature_cols].rename(columns={col: f\"{col}_{suffix}\" for col in feature_cols})\n",
    "    return result.reindex(df.index, fill_value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f332b9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Starting with 434 columns\n"
     ]
    }
   ],
   "source": [
    "# Start merging\n",
    "original_cols = len(df_merged.columns)\n",
    "print(f\"📊 Starting with {original_cols} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e219287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Merging primary entity features...\n",
      "  Merging card1...\n",
      "    Added 4 features\n",
      "  Merging addr1...\n",
      "    Added 4 features\n",
      "  Merging P_emaildomain...\n",
      "    Added 4 features\n",
      "✅ Primary entities merged. Total columns: 446\n"
     ]
    }
   ],
   "source": [
    "# Merge primary entities (most important)\n",
    "print(\"🔗 Merging primary entity features...\")\n",
    "\n",
    "for entity in primary_entities:\n",
    "    if entity in df_merged.columns:\n",
    "        print(f\"  Merging {entity}...\")\n",
    "        graph_features = merge_graph_features(df_merged, entity, entity.lower())\n",
    "        df_merged = pd.concat([df_merged, graph_features], axis=1)\n",
    "        print(f\"    Added {len(graph_features.columns)} features\")\n",
    "\n",
    "print(f\"✅ Primary entities merged. Total columns: {len(df_merged.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "024ea62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Merging secondary entity features...\n",
      "  Merging ProductCD...\n",
      "    Added 4 features\n",
      "  Merging DeviceType...\n",
      "    Added 4 features\n",
      "  Merging card4...\n",
      "    Added 4 features\n",
      "✅ Secondary entities merged. Total columns: 458\n"
     ]
    }
   ],
   "source": [
    "# Merge secondary entities\n",
    "print(\"🔗 Merging secondary entity features...\")\n",
    "\n",
    "for entity in secondary_entities:\n",
    "    if entity in df_merged.columns:\n",
    "        print(f\"  Merging {entity}...\")\n",
    "        graph_features = merge_graph_features(df_merged, entity, entity.lower())\n",
    "        df_merged = pd.concat([df_merged, graph_features], axis=1)\n",
    "        print(f\"    Added {len(graph_features.columns)} features\")\n",
    "\n",
    "print(f\"✅ Secondary entities merged. Total columns: {len(df_merged.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c23aa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Graph feature columns added (24):\n",
      "  - degree_centrality_card1\n",
      "  - pagerank_card1\n",
      "  - clustering_coefficient_card1\n",
      "  - betweenness_centrality_card1\n",
      "  - degree_centrality_addr1\n",
      "  - pagerank_addr1\n",
      "  - clustering_coefficient_addr1\n",
      "  - betweenness_centrality_addr1\n",
      "  - degree_centrality_p_emaildomain\n",
      "  - pagerank_p_emaildomain\n",
      "  - clustering_coefficient_p_emaildomain\n",
      "  - betweenness_centrality_p_emaildomain\n",
      "  - degree_centrality_productcd\n",
      "  - pagerank_productcd\n",
      "  - clustering_coefficient_productcd\n",
      "  - betweenness_centrality_productcd\n",
      "  - degree_centrality_devicetype\n",
      "  - pagerank_devicetype\n",
      "  - clustering_coefficient_devicetype\n",
      "  - betweenness_centrality_devicetype\n",
      "  - degree_centrality_card4\n",
      "  - pagerank_card4\n",
      "  - clustering_coefficient_card4\n",
      "  - betweenness_centrality_card4\n"
     ]
    }
   ],
   "source": [
    "# Show new graph feature columns\n",
    "graph_feature_cols = [col for col in df_merged.columns if any(metric in col for metric in ['degree_centrality', 'pagerank', 'clustering_coefficient', 'betweenness_centrality'])]\n",
    "print(f\"\\n📊 Graph feature columns added ({len(graph_feature_cols)}):\")\n",
    "for col in graph_feature_cols:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04a653b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Handling missing values from merging...\n",
      "📊 Missing values in graph features:\n",
      "Series([], dtype: int64)\n",
      "✅ Missing values filled with 0.0\n",
      "📊 Final merged dataset shape: (118108, 458)\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values from merging\n",
    "print(\"🔧 Handling missing values from merging...\")\n",
    "\n",
    "# Check for missing values in graph features\n",
    "missing_summary = df_merged[graph_feature_cols].isnull().sum()\n",
    "print(f\"📊 Missing values in graph features:\")\n",
    "print(missing_summary[missing_summary > 0])\n",
    "\n",
    "# Fill missing graph features with 0 (entities not in graph)\n",
    "df_merged[graph_feature_cols] = df_merged[graph_feature_cols].fillna(0.0)\n",
    "\n",
    "print(f\"✅ Missing values filled with 0.0\")\n",
    "print(f\"📊 Final merged dataset shape: {df_merged.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f06aef65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Hybrid Feature Summary:\n",
      "==================================================\n",
      "Original transaction features: 434\n",
      "Graph features added: 24\n",
      "Total features: 458\n",
      "\n",
      "📋 Sample of graph features:\n",
      "       degree_centrality_card1  pagerank_card1  clustering_coefficient_card1  \\\n",
      "count            118108.000000   118108.000000                 118108.000000   \n",
      "mean                 44.487240        0.000044                      0.636099   \n",
      "std                  30.917079        0.000030                      0.181294   \n",
      "min                   2.000000        0.000018                      0.000000   \n",
      "25%                  17.000000        0.000020                      0.481422   \n",
      "50%                  40.000000        0.000030                      0.628056   \n",
      "75%                  64.000000        0.000063                      0.763636   \n",
      "max                 121.000000        0.000126                      1.000000   \n",
      "\n",
      "       betweenness_centrality_card1  degree_centrality_addr1  pagerank_addr1  \\\n",
      "count                  1.181080e+05            118108.000000   118108.000000   \n",
      "mean                   1.206988e-06               597.772293        0.004948   \n",
      "std                    2.658049e-06               360.940691        0.003379   \n",
      "min                    0.000000e+00                 0.000000        0.000000   \n",
      "25%                    4.672738e-09               335.000000        0.002296   \n",
      "50%                    1.280735e-07               595.000000        0.004442   \n",
      "75%                    1.416979e-06               922.000000        0.007725   \n",
      "max                    2.150449e-05              1147.000000        0.010892   \n",
      "\n",
      "       clustering_coefficient_addr1  betweenness_centrality_addr1  \\\n",
      "count                 118108.000000                 118108.000000   \n",
      "mean                       0.049604                      0.002265   \n",
      "std                        0.061524                      0.001908   \n",
      "min                        0.000000                      0.000000   \n",
      "25%                        0.020700                      0.000658   \n",
      "50%                        0.038414                      0.001658   \n",
      "75%                        0.062377                      0.003509   \n",
      "max                        1.000000                      0.005716   \n",
      "\n",
      "       degree_centrality_p_emaildomain  pagerank_p_emaildomain  ...  \\\n",
      "count                    118108.000000           118108.000000  ...   \n",
      "mean                       2834.422004                0.032865  ...   \n",
      "std                        1976.838596                0.025172  ...   \n",
      "min                           0.000000                0.000000  ...   \n",
      "25%                         637.000000                0.004211  ...   \n",
      "50%                        3124.000000                0.034685  ...   \n",
      "75%                        4990.000000                0.061364  ...   \n",
      "max                        4990.000000                0.061364  ...   \n",
      "\n",
      "       clustering_coefficient_productcd  betweenness_centrality_productcd  \\\n",
      "count                     118108.000000                     118108.000000   \n",
      "mean                           0.006180                          0.141989   \n",
      "std                            0.007561                          0.076436   \n",
      "min                            0.002523                          0.002127   \n",
      "25%                            0.002523                          0.020137   \n",
      "50%                            0.002523                          0.186888   \n",
      "75%                            0.013353                          0.186888   \n",
      "max                            0.045778                          0.186888   \n",
      "\n",
      "       degree_centrality_devicetype  pagerank_devicetype  \\\n",
      "count                 118108.000000        118108.000000   \n",
      "mean                     705.407796             0.006653   \n",
      "std                     1281.087238             0.012223   \n",
      "min                        0.000000             0.000000   \n",
      "25%                        0.000000             0.000000   \n",
      "50%                        0.000000             0.000000   \n",
      "75%                        0.000000             0.000000   \n",
      "max                     3352.000000             0.032681   \n",
      "\n",
      "       clustering_coefficient_devicetype  betweenness_centrality_devicetype  \\\n",
      "count                      118108.000000                      118108.000000   \n",
      "mean                            0.002274                           0.009791   \n",
      "std                             0.004236                           0.018817   \n",
      "min                             0.000000                           0.000000   \n",
      "25%                             0.000000                           0.000000   \n",
      "50%                             0.000000                           0.000000   \n",
      "75%                             0.000000                           0.000000   \n",
      "max                             0.012604                           0.052533   \n",
      "\n",
      "       degree_centrality_card4  pagerank_card4  clustering_coefficient_card4  \\\n",
      "count            118108.000000   118108.000000                 118108.000000   \n",
      "mean               4160.992151        0.059356                      0.008523   \n",
      "std                1027.746326        0.015262                      0.037522   \n",
      "min                   0.000000        0.000000                      0.000000   \n",
      "25%                3081.000000        0.043342                      0.003682   \n",
      "50%                4847.000000        0.069541                      0.003682   \n",
      "75%                4847.000000        0.069541                      0.005665   \n",
      "max                4847.000000        0.069541                      0.365795   \n",
      "\n",
      "       betweenness_centrality_card4  \n",
      "count                 118108.000000  \n",
      "mean                       0.109833  \n",
      "std                        0.041861  \n",
      "min                        0.000000  \n",
      "25%                        0.058106  \n",
      "50%                        0.139744  \n",
      "75%                        0.139744  \n",
      "max                        0.139744  \n",
      "\n",
      "[8 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Feature summary\n",
    "print(\"📊 Hybrid Feature Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"Original transaction features: {original_cols}\")\n",
    "print(f\"Graph features added: {len(graph_feature_cols)}\")\n",
    "print(f\"Total features: {len(df_merged.columns)}\")\n",
    "\n",
    "# Show sample of graph features\n",
    "print(f\"\\n📋 Sample of graph features:\")\n",
    "print(df_merged[graph_feature_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6d1fa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Starting feature preprocessing (same as baseline)...\n",
      "✅ Features (X): (118108, 457)\n",
      "✅ Target (y): (118108,)\n",
      "✅ Target distribution: {0: 113975, 1: 4133}\n",
      "📊 Categorical columns: 31\n",
      "📊 Numerical columns: 426\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing (same as baseline)\n",
    "print(\"🔧 Starting feature preprocessing (same as baseline)...\")\n",
    "\n",
    "# Separate features and target\n",
    "target_col = 'isFraud'\n",
    "X = df_merged.drop(columns=[target_col])\n",
    "y = df_merged[target_col]\n",
    "\n",
    "print(f\"✅ Features (X): {X.shape}\")\n",
    "print(f\"✅ Target (y): {y.shape}\")\n",
    "print(f\"✅ Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"📊 Categorical columns: {len(categorical_cols)}\")\n",
    "print(f\"📊 Numerical columns: {len(numerical_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b24297f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Handling missing values...\n",
      "🔧 Applying frequency encoding...\n",
      "✅ Preprocessing completed\n",
      "📊 Final feature matrix: (118108, 457)\n",
      "📊 Memory usage: 411.80 MB\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values in numerical columns\n",
    "print(\"🔧 Handling missing values...\")\n",
    "X_numerical = X[numerical_cols].copy()\n",
    "X_numerical = X_numerical.fillna(-999)\n",
    "\n",
    "# Frequency encoding for categorical columns\n",
    "print(\"🔧 Applying frequency encoding...\")\n",
    "X_categorical = X[categorical_cols].copy()\n",
    "X_categorical = X_categorical.fillna('MISSING')\n",
    "\n",
    "for col in categorical_cols:\n",
    "    value_counts = X_categorical[col].value_counts()\n",
    "    X_categorical[col] = X_categorical[col].map(value_counts)\n",
    "    X_categorical[col] = X_categorical[col].fillna(0)\n",
    "\n",
    "# Combine features\n",
    "X_processed = pd.concat([X_numerical, X_categorical], axis=1)\n",
    "\n",
    "print(f\"✅ Preprocessing completed\")\n",
    "print(f\"📊 Final feature matrix: {X_processed.shape}\")\n",
    "print(f\"📊 Memory usage: {X_processed.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bec0f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✂️ Splitting data (same as baseline)...\n",
      "✅ Training set: 94486 samples (80.0%)\n",
      "✅ Validation set: 23622 samples (20.0%)\n",
      "✅ Training fraud rate: 0.0350\n",
      "✅ Validation fraud rate: 0.0350\n"
     ]
    }
   ],
   "source": [
    "# Data Split (same as baseline)\n",
    "print(\"✂️ Splitting data (same as baseline)...\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_processed, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"✅ Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X_processed)*100:.1f}%)\")\n",
    "print(f\"✅ Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X_processed)*100:.1f}%)\")\n",
    "print(f\"✅ Training fraud rate: {y_train.mean():.4f}\")\n",
    "print(f\"✅ Validation fraud rate: {y_val.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f42083a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training Hybrid XGBoost model...\n",
      "✅ Hybrid model initialized\n",
      "📊 Training on 457 features (including 24 graph features)\n"
     ]
    }
   ],
   "source": [
    "# Train Hybrid Model (same parameters as baseline)\n",
    "print(\"🚀 Training Hybrid XGBoost model...\")\n",
    "\n",
    "# Initialize with same parameters as baseline\n",
    "hybrid_model = xgb.XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "print(\"✅ Hybrid model initialized\")\n",
    "print(f\"📊 Training on {X_train.shape[1]} features (including {len(graph_feature_cols)} graph features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86e1a8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Starting hybrid model training...\n",
      "[0]\tvalidation_0-auc:0.79623\n",
      "[1]\tvalidation_0-auc:0.84421\n",
      "[2]\tvalidation_0-auc:0.85833\n",
      "[3]\tvalidation_0-auc:0.86202\n",
      "[4]\tvalidation_0-auc:0.86333\n",
      "[5]\tvalidation_0-auc:0.86961\n",
      "[6]\tvalidation_0-auc:0.87016\n",
      "[7]\tvalidation_0-auc:0.87275\n",
      "[8]\tvalidation_0-auc:0.87617\n",
      "[9]\tvalidation_0-auc:0.87790\n",
      "[10]\tvalidation_0-auc:0.87881\n",
      "[11]\tvalidation_0-auc:0.88120\n",
      "[12]\tvalidation_0-auc:0.88200\n",
      "[13]\tvalidation_0-auc:0.88364\n",
      "[14]\tvalidation_0-auc:0.88476\n",
      "[15]\tvalidation_0-auc:0.88517\n",
      "[16]\tvalidation_0-auc:0.88650\n",
      "[17]\tvalidation_0-auc:0.88656\n",
      "[18]\tvalidation_0-auc:0.88726\n",
      "[19]\tvalidation_0-auc:0.88864\n",
      "[20]\tvalidation_0-auc:0.88898\n",
      "[21]\tvalidation_0-auc:0.88950\n",
      "[22]\tvalidation_0-auc:0.89021\n",
      "[23]\tvalidation_0-auc:0.89101\n",
      "[24]\tvalidation_0-auc:0.89155\n",
      "[25]\tvalidation_0-auc:0.89193\n",
      "[26]\tvalidation_0-auc:0.89256\n",
      "[27]\tvalidation_0-auc:0.89313\n",
      "[28]\tvalidation_0-auc:0.89356\n",
      "[29]\tvalidation_0-auc:0.89464\n",
      "[30]\tvalidation_0-auc:0.89533\n",
      "[31]\tvalidation_0-auc:0.89524\n",
      "[32]\tvalidation_0-auc:0.89579\n",
      "[33]\tvalidation_0-auc:0.89597\n",
      "[34]\tvalidation_0-auc:0.89666\n",
      "[35]\tvalidation_0-auc:0.89741\n",
      "[36]\tvalidation_0-auc:0.89783\n",
      "[37]\tvalidation_0-auc:0.89822\n",
      "[38]\tvalidation_0-auc:0.89893\n",
      "[39]\tvalidation_0-auc:0.89900\n",
      "[40]\tvalidation_0-auc:0.89930\n",
      "[41]\tvalidation_0-auc:0.89997\n",
      "[42]\tvalidation_0-auc:0.90062\n",
      "[43]\tvalidation_0-auc:0.90107\n",
      "[44]\tvalidation_0-auc:0.90195\n",
      "[45]\tvalidation_0-auc:0.90204\n",
      "[46]\tvalidation_0-auc:0.90252\n",
      "[47]\tvalidation_0-auc:0.90345\n",
      "[48]\tvalidation_0-auc:0.90370\n",
      "[49]\tvalidation_0-auc:0.90403\n",
      "[50]\tvalidation_0-auc:0.90450\n",
      "[51]\tvalidation_0-auc:0.90486\n",
      "[52]\tvalidation_0-auc:0.90556\n",
      "[53]\tvalidation_0-auc:0.90616\n",
      "[54]\tvalidation_0-auc:0.90658\n",
      "[55]\tvalidation_0-auc:0.90683\n",
      "[56]\tvalidation_0-auc:0.90703\n",
      "[57]\tvalidation_0-auc:0.90741\n",
      "[58]\tvalidation_0-auc:0.90749\n",
      "[59]\tvalidation_0-auc:0.90778\n",
      "[60]\tvalidation_0-auc:0.90817\n",
      "[61]\tvalidation_0-auc:0.90851\n",
      "[62]\tvalidation_0-auc:0.90855\n",
      "[63]\tvalidation_0-auc:0.90901\n",
      "[64]\tvalidation_0-auc:0.90906\n",
      "[65]\tvalidation_0-auc:0.90912\n",
      "[66]\tvalidation_0-auc:0.90922\n",
      "[67]\tvalidation_0-auc:0.90928\n",
      "[68]\tvalidation_0-auc:0.90964\n",
      "[69]\tvalidation_0-auc:0.90971\n",
      "[70]\tvalidation_0-auc:0.91014\n",
      "[71]\tvalidation_0-auc:0.91029\n",
      "[72]\tvalidation_0-auc:0.91057\n",
      "[73]\tvalidation_0-auc:0.91088\n",
      "[74]\tvalidation_0-auc:0.91148\n",
      "[75]\tvalidation_0-auc:0.91182\n",
      "[76]\tvalidation_0-auc:0.91226\n",
      "[77]\tvalidation_0-auc:0.91237\n",
      "[78]\tvalidation_0-auc:0.91270\n",
      "[79]\tvalidation_0-auc:0.91287\n",
      "[80]\tvalidation_0-auc:0.91298\n",
      "[81]\tvalidation_0-auc:0.91317\n",
      "[82]\tvalidation_0-auc:0.91348\n",
      "[83]\tvalidation_0-auc:0.91343\n",
      "[84]\tvalidation_0-auc:0.91343\n",
      "[85]\tvalidation_0-auc:0.91354\n",
      "[86]\tvalidation_0-auc:0.91372\n",
      "[87]\tvalidation_0-auc:0.91383\n",
      "[88]\tvalidation_0-auc:0.91444\n",
      "[89]\tvalidation_0-auc:0.91488\n",
      "[90]\tvalidation_0-auc:0.91504\n",
      "[91]\tvalidation_0-auc:0.91499\n",
      "[92]\tvalidation_0-auc:0.91534\n",
      "[93]\tvalidation_0-auc:0.91555\n",
      "[94]\tvalidation_0-auc:0.91562\n",
      "[95]\tvalidation_0-auc:0.91587\n",
      "[96]\tvalidation_0-auc:0.91611\n",
      "[97]\tvalidation_0-auc:0.91626\n",
      "[98]\tvalidation_0-auc:0.91632\n",
      "[99]\tvalidation_0-auc:0.91678\n",
      "[100]\tvalidation_0-auc:0.91728\n",
      "[101]\tvalidation_0-auc:0.91746\n",
      "[102]\tvalidation_0-auc:0.91761\n",
      "[103]\tvalidation_0-auc:0.91782\n",
      "[104]\tvalidation_0-auc:0.91794\n",
      "[105]\tvalidation_0-auc:0.91801\n",
      "[106]\tvalidation_0-auc:0.91800\n",
      "[107]\tvalidation_0-auc:0.91827\n",
      "[108]\tvalidation_0-auc:0.91829\n",
      "[109]\tvalidation_0-auc:0.91836\n",
      "[110]\tvalidation_0-auc:0.91852\n",
      "[111]\tvalidation_0-auc:0.91865\n",
      "[112]\tvalidation_0-auc:0.91883\n",
      "[113]\tvalidation_0-auc:0.91899\n",
      "[114]\tvalidation_0-auc:0.91930\n",
      "[115]\tvalidation_0-auc:0.91930\n",
      "[116]\tvalidation_0-auc:0.91934\n",
      "[117]\tvalidation_0-auc:0.91949\n",
      "[118]\tvalidation_0-auc:0.91951\n",
      "[119]\tvalidation_0-auc:0.91983\n",
      "[120]\tvalidation_0-auc:0.91991\n",
      "[121]\tvalidation_0-auc:0.91993\n",
      "[122]\tvalidation_0-auc:0.91995\n",
      "[123]\tvalidation_0-auc:0.92031\n",
      "[124]\tvalidation_0-auc:0.92020\n",
      "[125]\tvalidation_0-auc:0.92023\n",
      "[126]\tvalidation_0-auc:0.92074\n",
      "[127]\tvalidation_0-auc:0.92063\n",
      "[128]\tvalidation_0-auc:0.92073\n",
      "[129]\tvalidation_0-auc:0.92089\n",
      "[130]\tvalidation_0-auc:0.92130\n",
      "[131]\tvalidation_0-auc:0.92133\n",
      "[132]\tvalidation_0-auc:0.92130\n",
      "[133]\tvalidation_0-auc:0.92133\n",
      "[134]\tvalidation_0-auc:0.92145\n",
      "[135]\tvalidation_0-auc:0.92178\n",
      "[136]\tvalidation_0-auc:0.92193\n",
      "[137]\tvalidation_0-auc:0.92254\n",
      "[138]\tvalidation_0-auc:0.92253\n",
      "[139]\tvalidation_0-auc:0.92270\n",
      "[140]\tvalidation_0-auc:0.92306\n",
      "[141]\tvalidation_0-auc:0.92306\n",
      "[142]\tvalidation_0-auc:0.92326\n",
      "[143]\tvalidation_0-auc:0.92344\n",
      "[144]\tvalidation_0-auc:0.92340\n",
      "[145]\tvalidation_0-auc:0.92339\n",
      "[146]\tvalidation_0-auc:0.92339\n",
      "[147]\tvalidation_0-auc:0.92346\n",
      "[148]\tvalidation_0-auc:0.92350\n",
      "[149]\tvalidation_0-auc:0.92375\n",
      "[150]\tvalidation_0-auc:0.92387\n",
      "[151]\tvalidation_0-auc:0.92386\n",
      "[152]\tvalidation_0-auc:0.92396\n",
      "[153]\tvalidation_0-auc:0.92404\n",
      "[154]\tvalidation_0-auc:0.92408\n",
      "[155]\tvalidation_0-auc:0.92416\n",
      "[156]\tvalidation_0-auc:0.92447\n",
      "[157]\tvalidation_0-auc:0.92471\n",
      "[158]\tvalidation_0-auc:0.92483\n",
      "[159]\tvalidation_0-auc:0.92477\n",
      "[160]\tvalidation_0-auc:0.92469\n",
      "[161]\tvalidation_0-auc:0.92497\n",
      "[162]\tvalidation_0-auc:0.92502\n",
      "[163]\tvalidation_0-auc:0.92498\n",
      "[164]\tvalidation_0-auc:0.92495\n",
      "[165]\tvalidation_0-auc:0.92505\n",
      "[166]\tvalidation_0-auc:0.92550\n",
      "[167]\tvalidation_0-auc:0.92545\n",
      "[168]\tvalidation_0-auc:0.92567\n",
      "[169]\tvalidation_0-auc:0.92575\n",
      "[170]\tvalidation_0-auc:0.92590\n",
      "[171]\tvalidation_0-auc:0.92592\n",
      "[172]\tvalidation_0-auc:0.92592\n",
      "[173]\tvalidation_0-auc:0.92628\n",
      "[174]\tvalidation_0-auc:0.92627\n",
      "[175]\tvalidation_0-auc:0.92626\n",
      "[176]\tvalidation_0-auc:0.92614\n",
      "[177]\tvalidation_0-auc:0.92632\n",
      "[178]\tvalidation_0-auc:0.92623\n",
      "[179]\tvalidation_0-auc:0.92613\n",
      "[180]\tvalidation_0-auc:0.92613\n",
      "[181]\tvalidation_0-auc:0.92618\n",
      "[182]\tvalidation_0-auc:0.92617\n",
      "[183]\tvalidation_0-auc:0.92629\n",
      "[184]\tvalidation_0-auc:0.92615\n",
      "[185]\tvalidation_0-auc:0.92616\n",
      "[186]\tvalidation_0-auc:0.92619\n",
      "✅ Hybrid model training completed!\n",
      "📊 Best iteration: 177\n",
      "📊 Best validation AUC: 0.9263\n"
     ]
    }
   ],
   "source": [
    "# Train the hybrid model\n",
    "print(\"🔥 Starting hybrid model training...\")\n",
    "\n",
    "hybrid_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"✅ Hybrid model training completed!\")\n",
    "print(f\"📊 Best iteration: {hybrid_model.best_iteration}\")\n",
    "print(f\"📊 Best validation AUC: {hybrid_model.best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "512d6b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Evaluating hybrid model performance...\n",
      "🎯 PERFORMANCE COMPARISON:\n",
      "============================================================\n",
      "Baseline ROC-AUC:     0.9213\n",
      "Hybrid ROC-AUC:       0.9263\n",
      "Improvement:          +0.0050\n",
      "Relative Improvement: +0.54%\n",
      "============================================================\n",
      "🎉 SUCCESS! Graph features improved the model!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and Compare Performance\n",
    "print(\"📊 Evaluating hybrid model performance...\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = hybrid_model.predict_proba(X_val)[:, 1]\n",
    "y_pred = hybrid_model.predict(X_val)\n",
    "\n",
    "# Calculate metrics\n",
    "hybrid_roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "\n",
    "print(\"🎯 PERFORMANCE COMPARISON:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Baseline ROC-AUC:     {BASELINE_ROC_AUC:.4f}\")\n",
    "print(f\"Hybrid ROC-AUC:       {hybrid_roc_auc:.4f}\")\n",
    "print(f\"Improvement:          {hybrid_roc_auc - BASELINE_ROC_AUC:+.4f}\")\n",
    "print(f\"Relative Improvement: {((hybrid_roc_auc - BASELINE_ROC_AUC) / BASELINE_ROC_AUC * 100):+.2f}%\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if hybrid_roc_auc > BASELINE_ROC_AUC:\n",
    "    print(\"🎉 SUCCESS! Graph features improved the model!\")\n",
    "else:\n",
    "    print(\"🤔 Graph features didn't improve performance. Need investigation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "598cc26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Detailed Performance Metrics:\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Legitimate       0.98      1.00      0.99     22795\n",
      "       Fraud       0.90      0.42      0.57       827\n",
      "\n",
      "    accuracy                           0.98     23622\n",
      "   macro avg       0.94      0.71      0.78     23622\n",
      "weighted avg       0.98      0.98      0.97     23622\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Detailed performance metrics\n",
    "print(\"📋 Detailed Performance Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_val, y_pred, target_names=['Legitimate', 'Fraud']))\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca5a08c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyzing feature importance...\n",
      "🏆 Top 20 Most Important Features:\n",
      "                         feature  importance\n",
      "297                         V258    0.210359\n",
      "240                         V201    0.051369\n",
      "228                         V189    0.050857\n",
      "18                            C8    0.026764\n",
      "109                          V70    0.019346\n",
      "108                          V69    0.017547\n",
      "129                          V90    0.016753\n",
      "323                         V284    0.015767\n",
      "130                          V91    0.014120\n",
      "24                           C14    0.012966\n",
      "284                         V245    0.010939\n",
      "347                         V308    0.010584\n",
      "333                         V294    0.007579\n",
      "22                           C12    0.007442\n",
      "245                         V206    0.006742\n",
      "211                         V172    0.006633\n",
      "87                           V48    0.006393\n",
      "414  degree_centrality_productcd    0.006177\n",
      "177                         V138    0.006157\n",
      "14                            C4    0.006142\n",
      "\n",
      "📊 Graph features in top 20: 1/20\n",
      "  #18: degree_centrality_productcd (importance: 0.0062)\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance Analysis\n",
    "print(\"🔍 Analyzing feature importance...\")\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_processed.columns,\n",
    "    'importance': hybrid_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"🏆 Top 20 Most Important Features:\")\n",
    "print(feature_importance.head(20))\n",
    "\n",
    "# Check how many graph features are in top 20\n",
    "top_20_features = feature_importance.head(20)['feature'].tolist()\n",
    "graph_features_in_top20 = [f for f in top_20_features if any(metric in f for metric in ['degree_centrality', 'pagerank', 'clustering_coefficient', 'betweenness_centrality'])]\n",
    "\n",
    "print(f\"\\n📊 Graph features in top 20: {len(graph_features_in_top20)}/20\")\n",
    "for gf in graph_features_in_top20:\n",
    "    rank = top_20_features.index(gf) + 1\n",
    "    importance = feature_importance[feature_importance['feature'] == gf]['importance'].iloc[0]\n",
    "    print(f\"  #{rank}: {gf} (importance: {importance:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9609068c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Creating feature importance visualization...\n",
      "✅ Feature importance plot saved as 'hybrid_feature_importance.png'\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance Visualization\n",
    "print(\"📊 Creating feature importance visualization...\")\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_features = feature_importance.head(20)\n",
    "\n",
    "# Color graph features differently\n",
    "colors = ['red' if any(metric in feat for metric in ['degree_centrality', 'pagerank', 'clustering_coefficient', 'betweenness_centrality']) else 'steelblue' for feat in top_features['feature']]\n",
    "\n",
    "bars = plt.barh(range(len(top_features)), top_features['importance'], color=colors)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Most Important Features (Hybrid Model)\\nRed = Graph Features, Blue = Traditional Features')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Add importance values on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.3f}', ha='left', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hybrid_feature_importance.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"✅ Feature importance plot saved as 'hybrid_feature_importance.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fe28f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving hybrid model...\n",
      "✅ Hybrid model saved to: ..\\app\\artifacts\\hybrid_model.pkl\n",
      "✅ Hybrid feature names saved to: ..\\app\\artifacts\\hybrid_feature_names.pkl\n",
      "✅ Hybrid feature importance saved to: ..\\app\\artifacts\\hybrid_feature_importance.csv\n",
      "\n",
      "🎉 HYBRID MODEL TRAINING COMPLETED!\n",
      "🏆 Final ROC-AUC Score: 0.9263\n",
      "📈 Improvement over baseline: +0.0050\n"
     ]
    }
   ],
   "source": [
    "# Save Final Model\n",
    "print(\"💾 Saving hybrid model...\")\n",
    "\n",
    "# Save the trained hybrid model\n",
    "model_path = output_dir / \"hybrid_model.pkl\"\n",
    "joblib.dump(hybrid_model, model_path)\n",
    "print(f\"✅ Hybrid model saved to: {model_path}\")\n",
    "\n",
    "# Save hybrid feature names\n",
    "feature_names_path = output_dir / \"hybrid_feature_names.pkl\"\n",
    "joblib.dump(X_processed.columns.tolist(), feature_names_path)\n",
    "print(f\"✅ Hybrid feature names saved to: {feature_names_path}\")\n",
    "\n",
    "# Save hybrid feature importance\n",
    "importance_path = output_dir / \"hybrid_feature_importance.csv\"\n",
    "feature_importance.to_csv(importance_path, index=False)\n",
    "print(f\"✅ Hybrid feature importance saved to: {importance_path}\")\n",
    "\n",
    "print(\"\\n🎉 HYBRID MODEL TRAINING COMPLETED!\")\n",
    "print(f\"🏆 Final ROC-AUC Score: {hybrid_roc_auc:.4f}\")\n",
    "print(f\"📈 Improvement over baseline: {hybrid_roc_auc - BASELINE_ROC_AUC:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3dd41f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 FINAL HYBRID MODEL SUMMARY:\n",
      "==================================================\n",
      "📊 Dataset size: 118,108 transactions\n",
      "📊 Total features: 457\n",
      "📊 Graph features: 24\n",
      "📊 Traditional features: 433\n",
      "📊 Fraud rate: 0.0350\n",
      "\n",
      "🎯 PERFORMANCE:\n",
      "Baseline ROC-AUC:  0.9213\n",
      "Hybrid ROC-AUC:    0.9263\n",
      "Improvement:       +0.0050\n",
      "Success:           ✅ YES\n",
      "\n",
      "🏆 Graph features impact:\n",
      "Graph features in top 20: 1/20\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Final Summary\n",
    "print(\"📋 FINAL HYBRID MODEL SUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"📊 Dataset size: {len(df_merged):,} transactions\")\n",
    "print(f\"📊 Total features: {X_processed.shape[1]:,}\")\n",
    "print(f\"📊 Graph features: {len(graph_feature_cols)}\")\n",
    "print(f\"📊 Traditional features: {X_processed.shape[1] - len(graph_feature_cols)}\")\n",
    "print(f\"📊 Fraud rate: {y.mean():.4f}\")\n",
    "print(\"\\n🎯 PERFORMANCE:\")\n",
    "print(f\"Baseline ROC-AUC:  {BASELINE_ROC_AUC:.4f}\")\n",
    "print(f\"Hybrid ROC-AUC:    {hybrid_roc_auc:.4f}\")\n",
    "print(f\"Improvement:       {hybrid_roc_auc - BASELINE_ROC_AUC:+.4f}\")\n",
    "print(f\"Success:           {'✅ YES' if hybrid_roc_auc > BASELINE_ROC_AUC else '❌ NO'}\")\n",
    "print(\"\\n🏆 Graph features impact:\")\n",
    "print(f\"Graph features in top 20: {len(graph_features_in_top20)}/20\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
